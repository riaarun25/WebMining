{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignments.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPua5tA3UZZkVgb6ewJ9LD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "YCmuWxCFwVJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme to tokenize the following using the NLTK toolkit:  \n",
        "a) word  \n",
        "b) sentence  \n",
        "c) remove stop words & punctuation and list the words  \n",
        "Note: Take the input as “What is Web Mining? Web Mining is the process of ‘’Data Mining” techniques, and extract information from Web documents and services. The main purpose of web mining is discovering useful information from the World-Wide Web and it’s usage patterns.”\n"
      ],
      "metadata": {
        "id": "mwH1jzCPwZln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"What is Web Mining? Web Mining is the process of 'Data Mining' techniques, and extract information from Web documents and services. The main purpose of web mining is discovering useful information from the World-Wide Web and it’s usage patterns.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "print(text_tokens)\n"
      ],
      "metadata": {
        "id": "H9bjyAW8wgJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence_data = \"What is Web Mining? Web Mining is the process of 'Data Mining' techniques, and extract information from Web documents and services. The main purpose of web mining is discovering useful information from the World-Wide Web and it’s usage patterns.\"\n",
        "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
        "print(nltk_tokens)\n"
      ],
      "metadata": {
        "id": "o59B4SJawnoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and punctuation and list the words:\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"What is Web Mining? Web Mining is the process of 'Data Mining' techniques, and extract information from Web documents and services. The main purpose of web mining is discovering useful information from the World-Wide Web and it’s usage patterns.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "print(tokens_without_sw)\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged_text = nltk.pos_tag(text_tokens)\n",
        "print(tagged_text)\n"
      ],
      "metadata": {
        "id": "QsqO4QIRwrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "uLshTkZWwyKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a python program to scrape website to extract the following:  \n",
        "a) raw HTML content  \n",
        "b) tags (title, p, a, div)  \n",
        "c) all textual content.  \n",
        "\n",
        "Note: Consider the input to be any website of your choice.\n"
      ],
      "metadata": {
        "id": "z5RpyuhYw2C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw HTML content\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html5lib')\n",
        "print(soup.prettify())\n"
      ],
      "metadata": {
        "id": "xjyyrhR8xHwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Title\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html5lib')\n",
        "print(soup.find('title').text)\n"
      ],
      "metadata": {
        "id": "LESiCHgYxMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p tag\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "texts = soup.find_all('p')\n",
        "for text in texts:\n",
        "  print(text.get_text())\n",
        "\n"
      ],
      "metadata": {
        "id": "Zkk_02qTxQGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a tag\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "texts = soup.find_all('a')\n",
        "for text in texts:\n",
        "  print(text.get_text())\n"
      ],
      "metadata": {
        "id": "ajF1jtppxUNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# div tag\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "texts = soup.find_all('div')\n",
        "for text in texts:\n",
        "  print(text.get_text())\n"
      ],
      "metadata": {
        "id": "sIiCaAXsxXdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All textual content\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "URL = \"https://www.geeksforgeeks.org/web-mining/\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "texts = soup.find_all()\n",
        "for text in texts:\n",
        "  print(text.get_text())\n"
      ],
      "metadata": {
        "id": "YXOAiGTFxcKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "uHge4ylexqiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme that performs Elias Gamma Encoding and Decoding for even numbers ranging from 1 to 20."
      ],
      "metadata": {
        "id": "cVtOMLRYxudN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Elias Gamma Encoding for all even numbers from 1 - 20\n",
        "from math import log\n",
        "log2 = lambda x: log(x, 2)\n",
        "def Unary(x):\n",
        "  return (x-1)*'0'+'1'\n",
        "\n",
        "def Binary(x, l=1):\n",
        "  s = '{0:0%db}' %l\n",
        "  return s.format(x)\n",
        "\n",
        "def Elias_Gamma(x):\n",
        "  if x==0:\n",
        "    return '0'\n",
        "\n",
        "  n = 1+int(log2(x))\n",
        "  b = x-2**(int(log2(x)))\n",
        "\n",
        "  l = int(log2(x))\n",
        "  \n",
        "  return Unary(n)+Binary(b,l)\n",
        "\n",
        "print(\"Elias Gamma Encoding for all even numbers from 1 - 20: \")\n",
        "for num in range(1, 20 + 1):\n",
        "      \n",
        "    if num % 2 == 0:\n",
        "      print(num, end = ' ')\n",
        "      print(\"=\", Elias_Gamma(num))\n"
      ],
      "metadata": {
        "id": "J2IgyxYOxvVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elias Gamma Decoding for all even numbers from 1 - 20\n",
        "\n",
        "import math\n",
        "  \n",
        "print(\"Elias Gamma Decoding for all even numbers from 1 - 20: \")\n",
        "def DecimalToBinary(num):\n",
        "     \n",
        "    if num >= 1:\n",
        "        DecimalToBinary(num // 2)\n",
        "    print(num % 2, end = '')\n",
        " \n",
        "if __name__ == '__main__':\n",
        "  def Elias_Gamma_Decoding(x):\n",
        "    x = list(x)\n",
        "    K = 0\n",
        "    while True:\n",
        "      if not x[K] == '0':\n",
        "        break\n",
        "        K = K + 1\n",
        "      \n",
        "    x = x[K:2*K+1]\n",
        "  \n",
        "    n = 0\n",
        "    x.reverse()\n",
        "      \n",
        "    for i in range(len(x)):\n",
        "        if x[i] == '1':\n",
        "            n = n+math.pow(2, i)\n",
        "    return int(n)\n",
        "    x = DecimalToBinary(num)\n",
        "    print(Elias_Gamma_Decoding(x))\n"
      ],
      "metadata": {
        "id": "sWKv2FXHx33w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4"
      ],
      "metadata": {
        "id": "qvhWiOKvyFdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme that uses TF-IDF to find the important words in the given corpus. \n",
        "\n",
        "Note: Collect strings from the following documents and create a corpus containing strings from documents d1, d2, and d3. \n",
        "\n",
        "•  d1: VIT Vellore University  \n",
        "•  d2: VIT  \n",
        "•  d3: Web  \n"
      ],
      "metadata": {
        "id": "iLibSUzdyMv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "d0 = 'VIT is one of the Top 10 Universities'\n",
        "d1 = 'Web mining is taught in VIT'\n",
        "d2 = 'Web makes up a large portion of the internet'\n",
        "d3 = 'Web mining is the  the application of data mining techniques'\n",
        "d4 = \"VIT is a private deemed university located in vellore\"\n",
        "string = [d0, d1, d2, d3, d4]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "result = tfidf.fit_transform(string)\n",
        "\n",
        "print('\\n Word indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "print(\"\\ntf-idf values: \")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "0TrvyK1ryIZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5"
      ],
      "metadata": {
        "id": "T2cmH0WByWOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme that performs Elias Delta Encoding and Decoding for a given number."
      ],
      "metadata": {
        "id": "sWLGf6YQyY9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elias Delta Encoding\n",
        "\n",
        "from math import log\n",
        "from math import floor\n",
        "\n",
        "def Binary_Representation_Without_MSB(x):\n",
        "    binary = \"{0:b}\".format(int(x))\n",
        "    binary_without_MSB = binary[1:]\n",
        "    return binary_without_MSB\n",
        " \n",
        "def EliasGammaEncode(k):\n",
        "    if (k == 0):\n",
        "        return '0'\n",
        "    N = 1 + floor(log(k, 2))\n",
        "    Unary = (N-1)*'0'+'1'\n",
        "    return Unary + Binary_Representation_Without_MSB(k)\n",
        " \n",
        "def EliasDeltaEncode(x):\n",
        "    Gamma = EliasGammaEncode(1 + floor(log(k, 2)))\n",
        "    binary_without_MSB = Binary_Representation_Without_MSB(k)\n",
        "    return Gamma+binary_without_MSB\n",
        "\n",
        "k = 10 \n",
        "print(EliasDeltaEncode(k))\n"
      ],
      "metadata": {
        "id": "-roPOctmyaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elias Delta Decoding\n",
        "\n",
        "import math\n",
        "  \n",
        "def  Elias_Delta_Decoding(x):\n",
        "    x = list(x)\n",
        "    L=0\n",
        "    while True:\n",
        "        if not x[L] == '0':\n",
        "            break\n",
        "        L= L + 1\n",
        "      \n",
        "    x=x[2*L+1:] \n",
        "      \n",
        "    x.insert(0,'1') \n",
        "    x.reverse()\n",
        "    n=0\n",
        "      \n",
        "    for i in range(len(x)): \n",
        "        if x[i]=='1':\n",
        "            n=n+math.pow(2,i)\n",
        "    return int(n)\n",
        "  \n",
        "x = '00100010'\n",
        "print(Elias_Delta_Decoding(x))\n"
      ],
      "metadata": {
        "id": "JgPvX5Jxyc8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6"
      ],
      "metadata": {
        "id": "jm1w-An8yoq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme to implement the Page Rank Algorithm in order to plot a graph and print the page rank for each page."
      ],
      "metadata": {
        "id": "IEDJtTqZzHYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pagerank(G, alpha=0.85, personalization=None,\n",
        "      max_iter=100, tol=1.0e-6, nstart=None, weight='weight',\n",
        "      dangling=None):\n",
        "\n",
        "  if len(G) == 0:\n",
        "    return {}\n",
        "\n",
        "  if not G.is_directed():\n",
        "    D = G.to_directed()\n",
        "  else:\n",
        "    D = G\n",
        "\n",
        "  W = nx.stochastic_graph(D, weight=weight)\n",
        "  N = W.number_of_nodes()\n",
        "\n",
        "  if nstart is None:\n",
        "    x = dict.fromkeys(W, 1.0 / N)\n",
        "  else:\n",
        "    s = float(sum(nstart.values()))\n",
        "    x = dict((k, v / s) for k, v in nstart.items())\n",
        "\n",
        "  if personalization is None:\n",
        "\n",
        "    p = dict.fromkeys(W, 1.0 / N)\n",
        "  else:\n",
        "    missing = set(G) - set(personalization)\n",
        "    if missing:\n",
        "      raise NetworkXError('Personalization dictionary '\n",
        "                'must have a value for every node. '\n",
        "                'Missing nodes %s' % missing)\n",
        "    s = float(sum(personalization.values()))\n",
        "    p = dict((k, v / s) for k, v in personalization.items())\n",
        "\n",
        "  if dangling is None:\n",
        "\n",
        "    dangling_weights = p\n",
        "  else:\n",
        "    missing = set(G) - set(dangling)\n",
        "    if missing:\n",
        "      raise NetworkXError('Dangling node dictionary '\n",
        "                'must have a value for every node. '\n",
        "                'Missing nodes %s' % missing)\n",
        "    s = float(sum(dangling.values()))\n",
        "    dangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
        "  dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
        "\n",
        "  for _ in range(max_iter):\n",
        "    xlast = x\n",
        "    x = dict.fromkeys(xlast.keys(), 0)\n",
        "    danglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
        "    for n in x:\n",
        "\n",
        "      for nbr in W[n]:\n",
        "        x[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
        "      x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
        "\n",
        "    err = sum([abs(x[n] - xlast[n]) for n in x])\n",
        "    if err < N*tol:\n",
        "      return x\n",
        "  raise NetworkXError('pagerank: power iteration failed to converge '\n",
        "            'in %d iterations.' % max_iter)\n",
        ">>> import networkx as nx\n",
        ">>> G=nx.barabasi_albert_graph(60,41)\n",
        ">>> pr=nx.pagerank(G,0.4)\n",
        ">>> pr\n"
      ],
      "metadata": {
        "id": "5Ba7V-NOyrn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7"
      ],
      "metadata": {
        "id": "1AHGYV5hzWwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme that uses the Networkx Module to implement the Hyperlink Induced Topic Search (HITS) Algorithm and prints the Hub and Authority scores."
      ],
      "metadata": {
        "id": "VZnxos3ozZOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "G.add_edges_from([('A', 'D'), ('B', 'C'), ('B', 'E'), ('C', 'A'),\n",
        "        ('D', 'C'), ('E', 'D'), ('E', 'B'), ('E', 'F'),\n",
        "        ('E', 'C'), ('F', 'C'), ('F', 'H'), ('G', 'A'),\n",
        "        ('G', 'C'), ('H', 'A')])\n",
        "\n",
        "plt.figure(figsize =(10, 10))\n",
        "nx.draw_networkx(G, with_labels = True)\n",
        "\n",
        "hubs, authorities = nx.hits(G, max_iter = 50, normalized = True)\n",
        "\n",
        "print(\"Hub Scores: \", hubs)\n",
        "print(\"Authority Scores: \", authorities)\n",
        "\n"
      ],
      "metadata": {
        "id": "_19KCdX2zZ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 8"
      ],
      "metadata": {
        "id": "RdE4PDL0zk3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme to implement the decision tree and prints the accuracy percentage, pression, recall and the predicted values."
      ],
      "metadata": {
        "id": "52prsSmRznXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function importing Dataset def importdata():\n",
        "balance_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning- '+'databases/balance-scale/balance-scale.data', sep= ',', header = None)\n",
        "\n",
        "# Printing the dataset shape\n",
        "print (\"Dataset Length: \", len(balance_data)) print (\"Dataset Shape: \", balance_data.shape)\n",
        "\n",
        "# Printing the dataset obseravtions print (\"Dataset: \",balance_data.head()) return balance_data\n",
        " \n",
        "# Function to split the dataset def splitdataset(balance_data):\n",
        "\n",
        "# Separating the target variable X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0]\n",
        "\n",
        "# Splitting the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100)\n",
        "\n",
        "return X, Y, X_train, X_test, y_train, y_test\n",
        "\n",
        "# Function to perform training with giniIndex. def train_using_gini(X_train, X_test, y_train):\n",
        "\n",
        "# Creating the classifier object\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "# Performing training clf_gini.fit(X_train, y_train) return clf_gini\n",
        "\n",
        "# Function to perform training with entropy.\n",
        "def tarin_using_entropy(X_train, X_test, y_train):\n",
        "\n",
        "# Decision tree with entropy\n",
        "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3, min_samples_leaf = 5)\n",
        "\n",
        "# Performing training clf_entropy.fit(X_train, y_train) return clf_entropy\n",
        "\n",
        "\n",
        "# Function to make predictions\n",
        "def prediction(X_test, clf_object):\n",
        "\n",
        "# Predicton on test with giniIndex y_pred = clf_object.predict(X_test) print(\"Predicted values:\") print(y_pred)\n",
        "return y_pred\n",
        " \n",
        "# Function to calculate accuracy def cal_accuracy(y_test, y_pred):\n",
        "\n",
        "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred)) print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) print(\"Report : \", classification_report(y_test, y_pred))\n",
        "# Driver code \n",
        "def main():\n",
        "\n",
        "# Building Phase data = importdata()\n",
        "X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train)\n",
        "\n",
        "# Operational Phase\n",
        "print(\"Results Using Gini Index:\")\n",
        "\n",
        "# Prediction using gini\n",
        "y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini)\n",
        "\n",
        "print(\"Results Using Entropy:\") # Prediction using entropy\n",
        "y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy)\n",
        "\n",
        "\n",
        "# Calling main function if name ==\" main__\":\n",
        "main()\n"
      ],
      "metadata": {
        "id": "3Ops_JeYzpyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 9"
      ],
      "metadata": {
        "id": "F2b8XZ8Yz5fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python programme that uses the K-means clustering algorithm and displays all clusters in different colours."
      ],
      "metadata": {
        "id": "slLtFbYmz8Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt import pandas as pd\n",
        "\n",
        "#Importing the mall dataset with pandas\n",
        "\n",
        "dataset = pd.read_csv('Mall_Customers.csv')\n",
        "X = dataset.iloc[:,[3,4]].values\n",
        "\n",
        "# Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans\n",
        "wcss =[]\n",
        "for i in range (1,11):\n",
        "kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter =300, n_init = 10, random_state = 0)\n",
        "kmeans.fit(X) wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the graph to visualize the Elbow Method to find the optimal number of cluster\n",
        "plt.plot(range(1,11),wcss) plt.title('The Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "# Applying K Means to the dataset with the optimal number of cluster\n",
        "\n",
        "kmeans=KMeans(n_clusters= 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
        "y_kmeans = kmeans.fit_predict(X) # Visualising the clusters\n",
        "plt.scatter(X[y_kmeans\n",
        "'Cluster 1')\t==\t0,\t0],\tX[y_kmeans\t==\t0,1],s\t=\t100,\tc='red', label =\t\n",
        "plt.scatter(X[y_kmeans 'Cluster 2')\t==\t1,\t0],\tX[y_kmeans\t==\t1,1],s\t=\t100,\tc='blue', label =\t\n",
        "plt.scatter(X[y_kmeans 'Cluster 3')\t==\t2,\t0],\tX[y_kmeans\t==\t2,1],s\t=\t100,\tc='green', label =\t\n",
        "plt.scatter(X[y_kmeans 'Cluster 4')\t==\t3,\t0],\tX[y_kmeans\t==\t3,1],s\t=\t100,\tc='cyan', label =\t\n",
        "plt.scatter(X[y_kmeans 'Cluster 5')\t==\t4,\t0],\tX[y_kmeans\t==\t4,1],s\t=\t100,\tc='magenta', label\t=\n",
        "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of clients') plt.xlabel('Annual Income (k$)') plt.ylabel('Spending score (1-100)') plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hTf4QzxL0GKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}